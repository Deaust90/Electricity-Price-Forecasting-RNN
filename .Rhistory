# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
# Fill leading/trailing NAs
complete_data$Price <- na.locf(complete_data$Price, na.rm = FALSE)
complete_data$Price <- na.locf(complete_data$Price, fromLast = TRUE, na.rm = FALSE) #To ensure start and end do not have NAs
# Remove duplicate timestamps
complete_data <- complete_data %>%
distinct(DateTime, .keep_all = TRUE)
# Verify no missing values
verify <- sum(is.na(complete_data$Price))  #Should return 0
target <- plot(y_train_scaled, type = "l", main = "Target over Time (Standardized)", ylab = "Scaled Price")
lookback <- 24
price_cap <- quantile(complete_data$Price, 0.995, na.rm = TRUE)
complete_data$Price[complete_data$Price > price_cap] <- price_cap #replacing extreme prices with a more reasonable ceiling.
data <- complete_data$Price
X <- matrix(nrow = length(data) - lookback, ncol = lookback)
Y <- c()
for (i in 1:(length(data) - lookback)) {
X[i,] <- data[i:(i + lookback - 1)]
Y[i] <- data[i + lookback]
}
train_size <- floor(0.8 * nrow(X))
x_train <- X[1:train_size, ]
y_train <- Y[1:train_size]
x_test <- X[(train_size + 1):nrow(X), ]
y_test <- Y[(train_size + 1):length(Y)]
standardized_data <- X.standardize(x_train, x_test)
x_train <- standardized_data[[1]]
x_test <- standardized_data[[2]]
y_mean <- mean(y_train)
y_sd <- sd(y_train)
y_train_scaled <- (y_train - y_mean) / y_sd
y_test_scaled  <- (y_test - y_mean) / y_sd
target <- plot(y_train_scaled, type = "l", main = "Target over Time (Standardized)", ylab = "Scaled Price")
abline(h = 0, col = "red", lty = 2)
lookback <- 24
price_cap <- quantile(complete_data$Price, 0.995, na.rm = TRUE)
complete_data$Price[complete_data$Price > price_cap] <- price_cap #replacing extreme prices with a more reasonable ceiling.
data <- complete_data$Price
X <- matrix(nrow = length(data) - lookback, ncol = lookback)
Y <- c()
for (i in 1:(length(data) - lookback)) {
X[i,] <- data[i:(i + lookback - 1)]
Y[i] <- data[i + lookback]
}
train_size <- floor(0.8 * nrow(X))
x_train <- X[1:train_size, ]
y_train <- Y[1:train_size]
x_test <- X[(train_size + 1):nrow(X), ]
y_test <- Y[(train_size + 1):length(Y)]
standardized_data <- X.standardize(x_train, x_test)
x_train <- standardized_data[[1]]
x_test <- standardized_data[[2]]
y_mean <- mean(y_train)
y_sd <- sd(y_train)
y_train_scaled <- (y_train - y_mean) / y_sd
y_test_scaled  <- (y_test - y_mean) / y_sd
target <- plot(y_train_scaled, type = "l", main = "Target over Time (Standardized)", ylab = "Scaled Price")
abline(h = 0, col = "red", lty = 2)
unlink("Project DLNN_cache", recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(dplyr)
library(lubridate)
library(zoo)
library(tensorflow)
library(keras)
library(tidyverse)
library(learnr)
X.standardize <- function(X_train, X_test){
# calculate the values for the mean and standard deviation
X_mean <- apply(X_train, 2, mean)
X_sd <- apply(X_train, 2, sd)
# standardize the data
X_train_std <- scale(X_train, center = X_mean, scale = X_sd)
X_test_std <- scale(X_test, center = X_mean, scale = X_sd)
return(list(X_train_std, X_test_std))
}
bgd <- function(weights, y, X, hid_n, out_n, nn, costderiv,
epochs = 3000, lr = 0.00001, ...){
for(i in 1:epochs){
deriv <- costderiv(weights, y, X, nn, hid_n, out_n, ...)
weights <- weights - lr*deriv
}
return(weights)
}
# Forward pass and loss for Wx
fwd_Wx <- function(Wx, H_prev, X_t, Y_t, Wh, Wo) {
Z_t <- Wh * H_prev + Wx * X_t
H_t <- tanh(Z_t)
Y_pred <- Wo * H_t
L <- 0.5 * (Y_pred - Y_t)^2
return(as.numeric(L))
}
# Analytical gradient dL/dWx
analytical_grad_Wx <- function(Wx, H_prev, X_t, Y_t, Wh, Wo) {
Z_t <- Wh * H_prev + Wx * X_t
H_t <- tanh(Z_t)
Y_pred <- Wo * H_t
dL_dYpred <- Y_pred - Y_t
dYpred_dHt <- Wo
dHt_dZt <- 1 - tanh(Z_t)^2
dZt_dWx <- X_t
return(dL_dYpred * dYpred_dHt * dHt_dZt * dZt_dWx)
}
# Forward pass and loss for Wo
fwd_Wo <- function(Wo, H_prev, X_t, Y_t, Wh, Wx) {
Z_t <- Wh * H_prev + Wx * X_t
H_t <- tanh(Z_t)
Y_pred <- Wo * H_t
L <- 0.5 * (Y_pred - Y_t)^2
return(as.numeric(L))
}
# Analytical gradient dL/dWo
analytical_grad_Wo <- function(Wo, H_prev, X_t, Y_t, Wh, Wx) {
Z_t <- Wh * H_prev + Wx * X_t
H_t <- tanh(Z_t)
Y_pred <- Wo * H_t
dL_dYpred <- Y_pred - Y_t
dYpred_dWo <- H_t
return(dL_dYpred * dYpred_dWo)
}
# Numerical gradient via finite differences
numerical_grad <- function(param, f, eps = 1e-5) {
f_plus  <- f(param + eps)
f_minus <- f(param - eps)
return((f_plus - f_minus) / (2 * eps))
}
# Example fixed values
Wh <- 0.5; Wx <- 0.3; Wo <- 0.7
H_prev <- 0.1; X_t <- 0.2; Y_t <- 0.05
# Check Wx gradient
num_grad_Wx <- numerical_grad(Wx, function(w) fwd_Wx(w, H_prev, X_t, Y_t, Wh, Wo))
ana_grad_Wx <- analytical_grad_Wx(Wx, H_prev, X_t, Y_t, Wh, Wo)
cat("Wx numerical gradient:", num_grad_Wx, "\n")
cat("Wx analytical gradient:", ana_grad_Wx, "\n\n")
# Check Wo gradient
num_grad_Wo <- numerical_grad(Wo, function(w) fwd_Wo(w, H_prev, X_t, Y_t, Wh, Wx))
ana_grad_Wo <- analytical_grad_Wo(Wo, H_prev, X_t, Y_t, Wh, Wx)
cat("Wo numerical gradient:", num_grad_Wo, "\n")
cat("Wo analytical gradient:", ana_grad_Wo, "\n")
knitr::opts_chunk$set(
echo = TRUE,
results = 'hide',
warning = FALSE,
message = FALSE
)
#source("C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\funcs_mlr.R")
csv_files <- list.files(
#path = "C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\data_rnn",
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task and Files-20250508/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
library(data.table)
library(dplyr)
library(lubridate)
library(zoo)
library(tensorflow)
library(keras)
library(tidyverse)
X.standardize <- function(X_train, X_test){
# calculate the values for the mean and standard deviation
X_mean <- apply(X_train, 2, mean)
X_sd <- apply(X_train, 2, sd)
# standardize the data
X_train_std <- scale(X_train, center = X_mean, scale = X_sd)
X_test_std <- scale(X_test, center = X_mean, scale = X_sd)
return(list(X_train_std, X_test_std))
}
bgd <- function(weights, y, X, hid_n, out_n, nn, costderiv,
epochs = 3000, lr = 0.00001, ...){
for(i in 1:epochs){
deriv <- costderiv(weights, y, X, nn, hid_n, out_n, ...)
weights <- weights - lr*deriv
}
return(weights)
}
#source("C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\funcs_mlr.R")
csv_files <- list.files(
#path = "C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\data_rnn",
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task and Files-20250508/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
filtered_data <- combined_data[combined_data$MapCode == "DE_LU", ]
# Convert datetime to POSIXct (UTC timezone)
filtered_data$DateTime <- as.POSIXct(
filtered_data$DateTime,
tz = "UTC"
)
# Define start and end dates
start_date <- as.POSIXct("2018-11-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2023-11-30 23:00:00", tz = "UTC")
# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
#source("C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\funcs_mlr.R")
csv_files <- list.files(
#path = "C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\data_rnn",
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task and Files-20250508/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
filtered_data <- combined_data[combined_data$MapCode == "DE_LU", ]
# Convert datetime to POSIXct (UTC timezone)
filtered_data$DateTime <- as.POSIXct(
filtered_data$DateTime,
tz = "UTC"
)
# Define start and end dates
start_date <- as.POSIXct("2018-11-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2023-11-30 23:00:00", tz = "UTC")
# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
knitr::opts_chunk$set(
echo = TRUE,
results = 'hide',
warning = FALSE,
message = FALSE
)
library(data.table)
library(dplyr)
library(lubridate)
library(zoo)
library(tensorflow)
library(keras)
library(tidyverse)
X.standardize <- function(X_train, X_test){
# calculate the values for the mean and standard deviation
X_mean <- apply(X_train, 2, mean)
X_sd <- apply(X_train, 2, sd)
# standardize the data
X_train_std <- scale(X_train, center = X_mean, scale = X_sd)
X_test_std <- scale(X_test, center = X_mean, scale = X_sd)
return(list(X_train_std, X_test_std))
}
bgd <- function(weights, y, X, hid_n, out_n, nn, costderiv,
epochs = 3000, lr = 0.00001, ...){
for(i in 1:epochs){
deriv <- costderiv(weights, y, X, nn, hid_n, out_n, ...)
weights <- weights - lr*deriv
}
return(weights)
}
#source("C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\funcs_mlr.R")
csv_files <- list.files(
#path = "C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\data_rnn",
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task and Files-20250508/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
filtered_data <- combined_data[combined_data$MapCode == "DE_LU", ]
# Convert datetime to POSIXct (UTC timezone)
filtered_data$DateTime <- as.POSIXct(
filtered_data$DateTime,
tz = "UTC"
)
# Define start and end dates
start_date <- as.POSIXct("2018-11-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2023-11-30 23:00:00", tz = "UTC")
# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
# Fill leading/trailing NAs
complete_data$Price <- na.locf(complete_data$Price, na.rm = FALSE)
# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
csv_files <- list.files(
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task and Files-20250508/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one data.table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
filtered_data <- combined_data[combined_data$MapCode == "DE_LU", ]
# Convert datetime to POSIXct (UTC timezone)
filtered_data$DateTime <- as.POSIXct(
filtered_data$DateTime,
tz = "UTC"
)
# Define start and end dates
start_date <- as.POSIXct("2018-11-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2023-11-30 23:00:00", tz = "UTC")
# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
# Fill leading/trailing NAs
complete_data$Price <- na.locf(complete_data$Price, na.rm = FALSE)
complete_data$Price <- na.locf(complete_data$Price, fromLast = TRUE, na.rm = FALSE) #To ensure start and end do not have NAs
# Remove duplicate timestamps (keep first occurrence)
complete_data <- complete_data %>%
distinct(DateTime, .keep_all = TRUE)
# Verify no missing values
sum(is.na(complete_data$Price))  #Should return 0
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
#source("C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\funcs_mlr.R")
csv_files <- list.files(
#path = "C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\data_rnn",
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task and Files-20250508/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
filtered_data <- combined_data[combined_data$MapCode == "DE_LU", ]
# Convert datetime to POSIXct (UTC timezone)
filtered_data$DateTime <- as.POSIXct(
filtered_data$DateTime,
tz = "UTC"
)
# Define start and end dates
start_date <- as.POSIXct("2018-11-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2023-11-30 23:00:00", tz = "UTC")
# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
gc()
library(data.table)
library(dplyr)
library(lubridate)
library(zoo)
library(tensorflow)
library(keras)
library(tidyverse)
X.standardize <- function(X_train, X_test){
# calculate the values for the mean and standard deviation
X_mean <- apply(X_train, 2, mean)
X_sd <- apply(X_train, 2, sd)
# standardize the data
X_train_std <- scale(X_train, center = X_mean, scale = X_sd)
X_test_std <- scale(X_test, center = X_mean, scale = X_sd)
return(list(X_train_std, X_test_std))
}
bgd <- function(weights, y, X, hid_n, out_n, nn, costderiv,
epochs = 3000, lr = 0.00001, ...){
for(i in 1:epochs){
deriv <- costderiv(weights, y, X, nn, hid_n, out_n, ...)
weights <- weights - lr*deriv
}
return(weights)
}
#source("C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\funcs_mlr.R")
csv_files <- list.files(
#path = "C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\data_rnn",
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task and Files-20250508/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
filtered_data <- combined_data[combined_data$MapCode == "DE_LU", ]
# Convert datetime to POSIXct (UTC timezone)
filtered_data$DateTime <- as.POSIXct(
filtered_data$DateTime,
tz = "UTC"
)
# Define start and end dates
start_date <- as.POSIXct("2018-11-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2023-11-30 23:00:00", tz = "UTC")
# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Fill leading/trailing NAs
complete_data$Price <- na.locf(complete_data$Price, na.rm = FALSE)
complete_data$Price <- na.locf(complete_data$Price, fromLast = TRUE, na.rm = FALSE) #To ensure start and end do not have NAs
csv_files <- list.files(
#path = "C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\data_rnn",
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task and Files-20250508/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
filtered_data <- combined_data[combined_data$MapCode == "DE_LU", ]
# Convert datetime to POSIXct (UTC timezone)
filtered_data$DateTime <- as.POSIXct(
filtered_data$DateTime,
tz = "UTC"
)
# Define start and end dates
start_date <- as.POSIXct("2018-11-01 00:00:00", tz = "UTC")
#source("C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\funcs_mlr.R")
csv_files <- list.files(
#path = "C:\\Users\\udwal\\Documents\\Studies_Viadrina\\semester_3\\DeepLearning\\project\\data_rnn",
path = "/Users/andrescadena/Library/CloudStorage/OneDrive-europa-uni.de/Deep-NN/Project Task/data_rnn",
pattern = "\\.csv$",
full.names = TRUE)
# Read and combine all CSV files into one table
combined_data <- rbindlist(
lapply(csv_files, fread), #fread: Similar to read.table but faster and more convenient. All controls such as sep, colClasses and nrows are automatically detected.
fill = TRUE,
use.names = TRUE)
filtered_data <- combined_data[combined_data$MapCode == "DE_LU", ]
# Convert datetime to POSIXct (UTC timezone)
filtered_data$DateTime <- as.POSIXct(
filtered_data$DateTime,
tz = "UTC"
)
# Define start and end dates
start_date <- as.POSIXct("2018-11-01 00:00:00", tz = "UTC")
end_date <- as.POSIXct("2023-11-30 23:00:00", tz = "UTC")
# Generate complete hourly sequence
full_time_seq <- data.frame(
DateTime = seq(start_date, end_date, by = "hour")
)
# Merge with complete time sequence
complete_data <- full_time_seq %>%
left_join(filtered_data, by = "DateTime") %>%
arrange(DateTime)
# Interpolate missing prices linearly
complete_data$Price <- na.approx(complete_data$Price)
# Fill leading/trailing NAs
complete_data$Price <- na.locf(complete_data$Price, na.rm = FALSE)
complete_data$Price <- na.locf(complete_data$Price, fromLast = TRUE, na.rm = FALSE) #To ensure start and end do not have NAs
# Remove duplicate timestamps
complete_data <- complete_data %>%
distinct(DateTime, .keep_all = TRUE)
# Verify no missing values
verify <- sum(is.na(complete_data$Price))  #Should return 0
